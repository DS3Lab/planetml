#!/bin/bash
#SBATCH --job-name=glm
#
#SBATCH --partition=sphinx
#SBATCH --exclude=sphinx[1-3]
#SBATCH --gres=gpu:1
#SBATCH --time=3:59:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=8G
#SBATCH --output=/afs/cs.stanford.edu/u/biyuan/exe_log/glm_%j.log

source activate base                          # Activate my conda python environment
cd /sailhome/biyuan/fm/new/gpt-home/     # Change directory

nvidia-smi

ls -l /sys/class/net/
netif=`echo "import os; print(sorted([x for x in os.listdir('/sys/class/net/') if x.startswith('en')])[0])" | python`
echo setting network interface to $netif
job_id=$1
echo job_id $job_id
export NCCL_SOCKET_IFNAME=$netif
export GLOO_SOCKET_IFNAME=$netif
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1

world_size=8
machine_size=8
n_gpu_per_machine=$((world_size/machine_size))

i=0
while :
do
  if [ "$i" -ge "$n_gpu_per_machine" ]; then
      break
  fi
  
  DIST_CONF="--pp-mode pipe_sync_sample_mask_token_pipe --pipeline-group-size $world_size --cuda-id $i"
  MODEL_CONF="--model-type glm --model-name /sailhome/biyuan/fm/models/glm-130b-new --num-iters 9999999999999"
  INFERENCE_CONF="--fp16  --budget 12400 --batch-size 24 --input-seq-length 512 --generate-seq-length 32 --micro-batch-size 1 --num-layers 9 --max-layers 70"
  COOR_CONF="--coordinator-server-ip 10.6.7.244 --working-directory /sailhome/biyuan/working_dir --profiling no-profiling --net-interface $netif  --job_id $job_id"

  python -u dist_batch_inference_w_httpclient.py $DIST_CONF $MODEL_CONF $INFERENCE_CONF $COOR_CONF &

  ((port++))
  ((i++))

done

wait
